{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('TREC_dataset/dev.csv')\n",
    "train_df = pd.read_csv('TREC_dataset/train.csv')\n",
    "test_df = pd.read_csv('TREC_dataset/test.csv')\n",
    "print(\"Training Size: \", train_df.shape[0])\n",
    "print(\"Development Size: \", dev_df.shape[0])\n",
    "print(\"Test Size: \", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/pretrained/word2vec_model.gensim'\n",
    "w2v_model = KeyedVectors.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = w2v_model.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "\n",
    "print(\"Vocab Size: \", vocab_size)\n",
    "print(\"Embedding Size: \", embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "1. Build a word index using a pre-trained word2vec model (w2v_model). It creates a dictionary where words (represented by e) are mapped to their corresponding indices (represented by i) from the key_to_index attribute of the word2vec model.\n",
    "\n",
    "2. Convert text data from three different data frames (train_df, dev_df, and test_df) into sequences of integers. Each word in the text is replaced by its corresponding index from the word index created earlier.\n",
    "\n",
    "3. Set a maximum sequence length (max_seq_length) and then pad the sequences to ensure that they all have the same length. \\\n",
    "    a. Padding is done with zeros (`0.`)\n",
    "\n",
    "4. Create a label encoder by mapping unique labels in the 'label-coarse' column of the training data frame to integers.\n",
    "\n",
    "5. Encode the labels for the training, development, and test data sets using this label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.word_index = {e: i for e,\n",
    "                        i in w2v_model.key_to_index.items()}\n",
    "\n",
    "# Convert text data to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad sequences to ensure they have the same length\n",
    "max_seq_length = 40  # Adjust as needed\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
    "dev_data = pad_sequences(dev_sequences, maxlen=max_seq_length, padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = {label: i for i, label in enumerate(train_df['label-coarse'].unique())}\n",
    "\n",
    "train_labels = np.array([label_encoder[label] for label in train_df['label-coarse']])\n",
    "dev_labels = np.array([label_encoder[label] for label in dev_df['label-coarse']])\n",
    "test_labels = np.array([label_encoder[label] for label in test_df['label-coarse']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM + CNN [FINALISED]\n",
    "\n",
    "### Step 1: Define Hyperparameter Grid\n",
    "\n",
    "1. Define a grid of hyperparameters to search, including:\n",
    "   - `embedding_size`: Different embedding sizes.\n",
    "   - `lstm_units`: Different LSTM units.\n",
    "   - `batch_size`: Different batch sizes.\n",
    "\n",
    "### Step 2: Logging and Model Directory Setup\n",
    "\n",
    "2. Create a directory structure to store model logs.\n",
    "3. Configure logging to track and save results to a log file.\n",
    "\n",
    "### Step 3: Model Training Loop\n",
    "\n",
    "In this step, we systematically train multiple models, each with different hyperparameter settings, to identify the best-performing configuration. The process is as follows:\n",
    "\n",
    "#### Substep 1: Hyperparameter Iteration\n",
    "\n",
    "4.1. Iterate through the predefined hyperparameter combinations.\n",
    "   - For each combination, we explore various settings for:\n",
    "     - `embedding_size`: The size of word embeddings.\n",
    "     - `lstm_units`: The number of LSTM units in the bidirectional LSTM layer.\n",
    "     - `batch_size`: The batch size used during training.\n",
    "\n",
    "#### Substep 2: Model Architecture\n",
    "\n",
    "4.2. Build the model architecture for the current hyperparameter combination. The architecture includes:\n",
    "   - Embedding Layer: Converts input sequences into dense vector representations.\n",
    "   - Bidirectional LSTM: A recurrent layer that captures contextual information bidirectionally.\n",
    "   - Convolutional Layer: Applies convolutional operations to capture local patterns.\n",
    "   - Global Max Pooling: Extracts the most relevant information from convolutional outputs.\n",
    "   - Dropout: Regularization technique to prevent overfitting.\n",
    "   - Dense Layers: Fully connected layers for classification.\n",
    "   \n",
    "#### Substep 3: Model Compilation\n",
    "\n",
    "4.3. Compile the model with the following configurations:\n",
    "   - Learning Rate Schedule: Uses an exponential decay schedule to adjust the learning rate.\n",
    "   - Optimizer: Utilizes the Adam optimizer for gradient descent.\n",
    "   \n",
    "#### Substep 4: Early Stopping\n",
    "\n",
    "4.4. Implement early stopping as a precautionary measure to prevent overfitting during training. Early stopping monitors the loss on the training set and stops training if the loss on the training set does not improve for a specified number of epochs.\n",
    "\n",
    "#### Substep 5: Class Weights\n",
    "\n",
    "4.5. Define class weights to address data imbalance issues. Class weights assign higher importance to underrepresented classes during training, helping the model better learn from imbalanced data.\n",
    "\n",
    "#### Substep 6: Training and Validation\n",
    "\n",
    "4.6. Train the model on the training dataset with the specified hyperparameters. During training, the model learns to make predictions based on input sequences. Validation is performed on a separate development dataset to assess the model's performance during training.\n",
    "\n",
    "By systematically exploring different hyperparameter combinations and training models with varying configurations, we aim to identify the best-performing model with the most suitable hyperparameters for the text classification task.\n",
    "\n",
    "\n",
    "### Step 4: Model Saving\n",
    "\n",
    "5. After training, evaluate the model on the test set.\n",
    "6. Save the model in a directory named based on its test accuracy (rounded to four decimal places).\n",
    "7. Record additional information in a JSON file, including the model summary.\n",
    "8. Log the saved model path and version.\n",
    "\n",
    "Summary\n",
    "\n",
    "This code demonstrates a systematic approach to hyperparameter tuning and model saving for text classification tasks, ensuring reproducibility and easy tracking of model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import logging\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, Dropout, Dense, Embedding, Conv1D,LSTM,BatchNormalization\n",
    "\n",
    "# Create a directory for model logs\n",
    "if not os.path.exists('model_logs'):\n",
    "    os.mkdir('model_logs')\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'embedding_size': [300],# Different embedding sizes\n",
    "    'lstm_units': [32],        # Different LSTM units\n",
    "    'batch_size': [128],         # Different batch sizes\n",
    "}\n",
    "\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "model_folder = 'BiLSTM'\n",
    "\n",
    "# Create a directory for the current model\n",
    "model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "if not os.path.exists(model_logs_dir):\n",
    "    os.mkdir(model_logs_dir)\n",
    "\n",
    "# Configure logging to save results to a single log file\n",
    "log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "logging.basicConfig(filename=log_filepath,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Iterate through the parameter combinations\n",
    "for params in param_combinations:\n",
    "    embedding_size = params['embedding_size']\n",
    "    lstm_units = params['lstm_units']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    print(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    \n",
    "    \n",
    "    bi_lstm = tf.keras.Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_seq_length, trainable=False),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        Conv1D(128, 5, activation='relu', padding='same'),  # Convolutional layer\n",
    "        GlobalMaxPooling1D(),  # Global Max Pooling\n",
    "        Dropout(0.2),  # Dropout for regularization\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # Adding L2 regularization\n",
    "        BatchNormalization(),  # Batch normalization layer Adding another layer with L2 regularization\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "    initial_learning_rate = 0.01\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    bi_lstm.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define EarlyStopping callback to prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    class_weights = {0: 0.454, 1: 0.869, 2: 13.042, 3: 1.228, 4: 1.312}\n",
    "    # Train the model\n",
    "    history = bi_lstm.fit(train_data, train_labels, epochs=50,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(dev_data, dev_labels),\n",
    "                        callbacks=[early_stopping],\n",
    "                        # workers=14\n",
    "                        # class_weight=class_weights\n",
    "                        )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = bi_lstm.evaluate(\n",
    "        test_data, test_labels, verbose=2)\n",
    "    \n",
    "    model_path = os.path.join(\n",
    "        model_logs_dir, f'model_{round(test_accuracy,4)}')\n",
    "\n",
    "    # Save the entire model\n",
    "    tf.keras.models.save_model(bi_lstm, model_path)\n",
    "\n",
    "    # Record additional information in a JSON file\n",
    "    info = {\n",
    "        'Model': str(bi_lstm.summary()),\n",
    "        'Hyperparameters': params,\n",
    "        'Test Loss': test_loss,\n",
    "        'Test Accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "    info_path = os.path.join(\n",
    "        model_path, f'model_{round(test_accuracy,4)}_info.json')\n",
    "    with open(info_path, 'w') as info_file:\n",
    "        json.dump(info, info_file)\n",
    "\n",
    "    # Logging: You can log the saved model path and version\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "    # Log the results in the same log file\n",
    "    model_summary = []\n",
    "    bi_lstm.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_architecture = \"\\n\".join(model_summary)\n",
    "    logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "    logging.info(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    logging.info(f\"Test Loss: {test_loss}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + GRU + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(Attention, self).__init__()\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W_q = self.add_weight(\"W_q\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "#         self.W_k = self.add_weight(\"W_k\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "#         self.W_v = self.add_weight(\"W_v\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         Q = tf.matmul(inputs, self.W_q)\n",
    "#         K = tf.matmul(inputs, self.W_k)\n",
    "#         V = tf.matmul(inputs, self.W_v)\n",
    "\n",
    "#         attention_scores = tf.matmul(Q, K, transpose_b=True)\n",
    "#         attention_scores = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "#         output = tf.matmul(attention_scores, V)\n",
    "#         return output\n",
    "\n",
    "# # Define a grid of hyperparameters to search\n",
    "# param_grid = {\n",
    "#     'embedding_size': [150],  # Different embedding sizes\n",
    "#     'lstm_units': [64],      # Different LSTM units\n",
    "#     'gru_units': [32],        # Different GRU units\n",
    "#     'batch_size': [64],      # Different batch sizes\n",
    "# }\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "# print('Param Combinations: ',param_combinations)\n",
    "# model_folder = 'Ensemble_Model'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Iterate through the parameter combinations\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     gru_units = params['gru_units']\n",
    "#     batch_size = params['batch_size']\n",
    "\n",
    "    \n",
    "#     print(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, GRU Units={gru_units}, Batch Size={batch_size}\")\n",
    "\n",
    "#     # Define your model with attention\n",
    "#     # Define your simpler model architecture\n",
    "#     attention_LGRB = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(\n",
    "#             input_dim=vocab_size,\n",
    "#             output_dim=embedding_size,\n",
    "#             input_length=max_seq_length,\n",
    "#             trainable=False\n",
    "#         ),\n",
    "#         # Dropout for the embedding layer\n",
    "#         tf.keras.layers.SpatialDropout1D(0.2),\n",
    "#         tf.keras.layers.LSTM(lstm_units, return_sequences=True,\n",
    "#                              kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # L2 regularization\n",
    "#         tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "#         tf.keras.layers.GRU(gru_units, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "#         Attention(),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(\n",
    "#             0.001)),  # L2 regularization\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(5, activation='softmax')\n",
    "#     ])\n",
    "\n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.1\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "#     attention_LGRB.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = attention_LGRB.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),)\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = attention_LGRB.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "\n",
    "#     # Log the results in the same log file\n",
    "#     model_summary = []\n",
    "#     attention_LGRB.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#     model_architecture = \"\\n\".join(model_summary)\n",
    "#     logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, GRU Units={gru_units}, Batch Size={batch_size}\")\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param Combinations:\n",
      "{'attention_units': 64, 'batch_size': 64, 'conv_depth': 1, 'conv_width': 128, 'embedding_size': 100, 'kernel_size': 5}\n",
      "Testing hyperparameters: Embedding Size=100, Conv Depth=1, Conv Width=128, Kernel Size=5, Batch Size=64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 10s 104ms/step - loss: 1.2451 - accuracy: 0.5151 - val_loss: 2.4075 - val_accuracy: 0.4120\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 5s 63ms/step - loss: 0.7603 - accuracy: 0.6914 - val_loss: 1.5487 - val_accuracy: 0.4120\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 0.6380 - accuracy: 0.7452 - val_loss: 1.5762 - val_accuracy: 0.4120\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 5s 60ms/step - loss: 0.5308 - accuracy: 0.7912 - val_loss: 1.1434 - val_accuracy: 0.4940\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 0.4345 - accuracy: 0.8384 - val_loss: 1.1968 - val_accuracy: 0.6300\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 0.3569 - accuracy: 0.8738 - val_loss: 0.7730 - val_accuracy: 0.6920\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 0.3020 - accuracy: 0.8924 - val_loss: 0.6341 - val_accuracy: 0.7400\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 0.2695 - accuracy: 0.9067 - val_loss: 1.1307 - val_accuracy: 0.7360\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 0.2215 - accuracy: 0.9214 - val_loss: 0.7281 - val_accuracy: 0.7680\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 4s 56ms/step - loss: 0.1878 - accuracy: 0.9431 - val_loss: 0.8913 - val_accuracy: 0.7780\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 3s 41ms/step - loss: 0.1960 - accuracy: 0.9382 - val_loss: 0.9269 - val_accuracy: 0.7780\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 5s 62ms/step - loss: 0.1480 - accuracy: 0.9519 - val_loss: 1.5545 - val_accuracy: 0.7820\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 5s 58ms/step - loss: 0.1441 - accuracy: 0.9538 - val_loss: 0.9363 - val_accuracy: 0.7600\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 0.1060 - accuracy: 0.9673 - val_loss: 1.1566 - val_accuracy: 0.7860\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 3s 38ms/step - loss: 0.1441 - accuracy: 0.9568 - val_loss: 1.1738 - val_accuracy: 0.7780\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 3s 40ms/step - loss: 0.1141 - accuracy: 0.9630 - val_loss: 1.3533 - val_accuracy: 0.7800\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 4s 58ms/step - loss: 0.0961 - accuracy: 0.9691 - val_loss: 1.5242 - val_accuracy: 0.7760\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 0.0877 - accuracy: 0.9713 - val_loss: 1.6340 - val_accuracy: 0.7840\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.1342 - accuracy: 0.9641 - val_loss: 1.4348 - val_accuracy: 0.7680\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 3s 40ms/step - loss: 0.1397 - accuracy: 0.9580 - val_loss: 1.2773 - val_accuracy: 0.7740\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.1397 - accuracy: 0.9614 - val_loss: 1.1513 - val_accuracy: 0.8000\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.1020 - accuracy: 0.9687 - val_loss: 1.1749 - val_accuracy: 0.7960\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 5s 62ms/step - loss: 0.0701 - accuracy: 0.9778 - val_loss: 1.4802 - val_accuracy: 0.8000\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 5s 60ms/step - loss: 0.0681 - accuracy: 0.9812 - val_loss: 1.7428 - val_accuracy: 0.7840\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.0633 - accuracy: 0.9808 - val_loss: 1.5089 - val_accuracy: 0.7780\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 3s 42ms/step - loss: 0.0716 - accuracy: 0.9792 - val_loss: 1.7352 - val_accuracy: 0.8040\n",
      "Epoch 27/50\n",
      "78/78 [==============================] - 3s 41ms/step - loss: 0.0726 - accuracy: 0.9784 - val_loss: 1.9986 - val_accuracy: 0.7840\n",
      "Epoch 28/50\n",
      "78/78 [==============================] - 4s 58ms/step - loss: 0.0594 - accuracy: 0.9838 - val_loss: 1.8623 - val_accuracy: 0.7940\n",
      "Epoch 29/50\n",
      "78/78 [==============================] - 3s 38ms/step - loss: 0.0654 - accuracy: 0.9792 - val_loss: 1.8902 - val_accuracy: 0.8000\n",
      "Epoch 30/50\n",
      "78/78 [==============================] - 3s 38ms/step - loss: 0.1015 - accuracy: 0.9733 - val_loss: 1.4008 - val_accuracy: 0.7880\n",
      "Epoch 31/50\n",
      "78/78 [==============================] - 3s 40ms/step - loss: 0.0718 - accuracy: 0.9808 - val_loss: 1.5757 - val_accuracy: 0.8120\n",
      "Epoch 32/50\n",
      "78/78 [==============================] - 5s 61ms/step - loss: 0.0513 - accuracy: 0.9869 - val_loss: 1.9965 - val_accuracy: 0.7860\n",
      "Epoch 33/50\n",
      "78/78 [==============================] - 5s 61ms/step - loss: 0.0467 - accuracy: 0.9875 - val_loss: 2.1290 - val_accuracy: 0.8000\n",
      "Epoch 34/50\n",
      "78/78 [==============================] - 3s 37ms/step - loss: 0.0503 - accuracy: 0.9861 - val_loss: 2.6457 - val_accuracy: 0.7960\n",
      "Epoch 35/50\n",
      "78/78 [==============================] - 3s 38ms/step - loss: 0.0564 - accuracy: 0.9816 - val_loss: 2.2364 - val_accuracy: 0.7860\n",
      "Epoch 36/50\n",
      "78/78 [==============================] - 3s 38ms/step - loss: 0.0592 - accuracy: 0.9828 - val_loss: 2.1802 - val_accuracy: 0.7920\n",
      "Epoch 37/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.0731 - accuracy: 0.9822 - val_loss: 1.4833 - val_accuracy: 0.7840\n",
      "Epoch 38/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.0531 - accuracy: 0.9873 - val_loss: 1.4903 - val_accuracy: 0.7980\n",
      "Epoch 39/50\n",
      "78/78 [==============================] - 5s 62ms/step - loss: 0.0371 - accuracy: 0.9887 - val_loss: 1.6309 - val_accuracy: 0.7960\n",
      "Epoch 40/50\n",
      "78/78 [==============================] - 4s 58ms/step - loss: 0.0470 - accuracy: 0.9891 - val_loss: 2.1243 - val_accuracy: 0.7900\n",
      "Epoch 41/50\n",
      "78/78 [==============================] - 3s 36ms/step - loss: 0.0726 - accuracy: 0.9830 - val_loss: 1.6273 - val_accuracy: 0.7800\n",
      "Epoch 42/50\n",
      "78/78 [==============================] - 3s 37ms/step - loss: 0.0495 - accuracy: 0.9867 - val_loss: 1.7114 - val_accuracy: 0.7980\n",
      "Epoch 43/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.0648 - accuracy: 0.9816 - val_loss: 1.7724 - val_accuracy: 0.7800\n",
      "Epoch 44/50\n",
      "78/78 [==============================] - 5s 64ms/step - loss: 0.0368 - accuracy: 0.9893 - val_loss: 1.5792 - val_accuracy: 0.8040\n",
      "Epoch 45/50\n",
      "78/78 [==============================] - 5s 71ms/step - loss: 0.0340 - accuracy: 0.9897 - val_loss: 2.0816 - val_accuracy: 0.7840\n",
      "Epoch 46/50\n",
      "78/78 [==============================] - 3s 36ms/step - loss: 0.0352 - accuracy: 0.9863 - val_loss: 2.4638 - val_accuracy: 0.7920\n",
      "Epoch 47/50\n",
      "78/78 [==============================] - 3s 36ms/step - loss: 0.0395 - accuracy: 0.9889 - val_loss: 2.2041 - val_accuracy: 0.7880\n",
      "Epoch 48/50\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 0.0298 - accuracy: 0.9919 - val_loss: 2.4122 - val_accuracy: 0.8060\n",
      "Epoch 49/50\n",
      "78/78 [==============================] - 3s 39ms/step - loss: 0.0483 - accuracy: 0.9889 - val_loss: 2.0373 - val_accuracy: 0.7780\n",
      "Epoch 50/50\n",
      "78/78 [==============================] - 3s 40ms/step - loss: 0.0518 - accuracy: 0.9859 - val_loss: 1.9163 - val_accuracy: 0.7920\n",
      "16/16 - 0s - loss: 2.2004 - accuracy: 0.8080 - 81ms/epoch - 5ms/step\n",
      "INFO:tensorflow:Assets written to: model_logs/CNN/model_0.808/assets\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 40)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 40, 100)              3000000   ['input_2[0][0]']             \n",
      "                                                          00                                      \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 40, 128)              38528     ['embedding_2[0][0]']         \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 40, 128)              65664     ['conv1d_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 40, 128)              82048     ['conv1d_4[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 40, 128)              512       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 40, 128)              512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 40, 128)              512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 40, 384)              0         ['batch_normalization_3[0][0]'\n",
      " )                                                                  , 'batch_normalization_4[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Gl  (None, 384)                  0         ['concatenate_2[0][0]']       \n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1, 384)               0         ['global_max_pooling1d_2[0][0]\n",
      " )                                                                  ']                            \n",
      "                                                                                                  \n",
      " attention_1 (Attention)     ((None, 384),                49345     ['tf.expand_dims[0][0]']      \n",
      "                              (None, 1, 1))                                                       \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 128)                  49280     ['attention_1[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 128)                  0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 5)                    645       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 300287046 (1.12 GB)\n",
      "Trainable params: 286278 (1.09 MB)\n",
      "Non-trainable params: 300000768 (1.12 GB)\n",
      "__________________________________________________________________________________________________\n",
      "Saved model: model_logs/CNN/model_0.808\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from tensorflow.keras.layers import Input, Conv1D,BatchNormalization, GlobalMaxPooling1D, Dropout, Dense, Embedding, Concatenate\n",
    "\n",
    "\n",
    "# class Attention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.W1 = tf.keras.layers.Dense(units)\n",
    "#         self.W2 = tf.keras.layers.Dense(units)\n",
    "#         self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "#     def call(self, features):\n",
    "#         query_value_attention_score = self.V(tf.nn.tanh(\n",
    "#             self.W1(features) + self.W2(features)))\n",
    "#         attention_weights = tf.nn.softmax(query_value_attention_score, axis=1)\n",
    "#         context_vector = attention_weights * features\n",
    "#         context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "#         return context_vector, attention_weights\n",
    "\n",
    "# # Updated hyperparameter grid to include depth and width\n",
    "# param_grid = {\n",
    "#     'embedding_size': [100],     # Embedding sizes\n",
    "#     'conv_depth': [1],          # Number of Conv1D layers (Depth)\n",
    "#     'conv_width': [128],    # Number of filters in each Conv1D layer (Width)\n",
    "#     'kernel_size': [5],         # Kernel sizes\n",
    "#     'batch_size': [64],      # Batch sizes\n",
    "#     'attention_units': [64]\n",
    "# }\n",
    "\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "# print('Param Combinations:')\n",
    "# for p in param_combinations:\n",
    "#     print(p)\n",
    "\n",
    "# model_folder = 'CNN'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # In the model architecture:\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     conv_depth = params['conv_depth']\n",
    "#     conv_width = params['conv_width']\n",
    "#     kernel_size = params['kernel_size']\n",
    "#     batch_size = params['batch_size']\n",
    "#     attention_units = params['attention_units']\n",
    "#     # Log the current hyperparameters\n",
    "#     print(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Depth={conv_depth}, Conv Width={conv_width}, Kernel Size={kernel_size}, Batch Size={batch_size}\")\n",
    "\n",
    "#     # -------------------------------------------------------------------------------------\n",
    "#     # input_layer = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "#     # embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "#     #                             output_dim=embedding_size,\n",
    "#     #                             input_length=max_seq_length,\n",
    "#     #                             trainable=False)(input_layer)\n",
    "#     # conv_layer = tf.keras.layers.Conv1D(conv_width, 5, activation='relu')(embedding_layer)\n",
    "#     # context_vector, attention_weights = Attention(attention_units)(conv_layer)\n",
    "#     # dense_layer = tf.keras.layers.Dense(64, activation='relu')(context_vector)\n",
    "#     # dropout_layer = tf.keras.layers.Dropout(0.6)(dense_layer)\n",
    "#     # output_layer = tf.keras.layers.Dense(5, activation='softmax')(dropout_layer)\n",
    "\n",
    "#     # conv_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "#     # -------------------------------------------------------------------------------------\n",
    "#     input_layer = Input(shape=(max_seq_length,))\n",
    "#     embedding_layer = Embedding(input_dim=vocab_size,\n",
    "#                     output_dim=embedding_size,\n",
    "#                     input_length=max_seq_length,\n",
    "#                     trainable=False)(input_layer)\n",
    "#     conv_layer_1 = Conv1D(filters=conv_width, kernel_size=3, activation='relu', padding='same')(embedding_layer)\n",
    "#     conv_layer_2 = Conv1D(filters=conv_width, kernel_size=4, activation='relu', padding='same')(conv_layer_1)\n",
    "#     conv_layer_3 = Conv1D(filters=conv_width, kernel_size=5, activation='relu', padding='same')(conv_layer_2)\n",
    "#     bn_layer_1 = BatchNormalization()(conv_layer_1)\n",
    "#     bn_layer_2 = BatchNormalization()(conv_layer_2)\n",
    "#     bn_layer_3 = BatchNormalization()(conv_layer_3)\n",
    "#     concat_layer = Concatenate()([bn_layer_1, bn_layer_2, bn_layer_3])\n",
    "#     pooling_layer = GlobalMaxPooling1D()(concat_layer)\n",
    "#     context_vector, attention_weights = Attention(attention_units)(tf.expand_dims(pooling_layer, 1))\n",
    "#     dense_layer = Dense(128, activation='relu')(context_vector)\n",
    "#     dropout_layer = Dropout(0.5)(dense_layer)\n",
    "#     output_layer = Dense(5, activation='softmax')(dropout_layer)\n",
    "    \n",
    "#     conv_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.01\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "   \n",
    "#     conv_model.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = conv_model.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),\n",
    "#                         callbacks=[early_stopping],\n",
    "#                         workers=8,\n",
    "#                         )\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = conv_model.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Depth={conv_depth}, Conv Width={conv_width}, Kernel Size={kernel_size}, Batch Size={batch_size}\")\n",
    "    \n",
    "#     model_path = os.path.join(model_logs_dir, f'model_{round(test_accuracy,4)}')\n",
    "\n",
    "#     # Save the entire model\n",
    "#     tf.keras.models.save_model(conv_model, model_path)\n",
    "\n",
    "#     # Record additional information in a JSON file\n",
    "#     info = {\n",
    "#         'Model':str(conv_model.summary()),\n",
    "#         'Hyperparameters': params,\n",
    "#         'Test Loss': test_loss,\n",
    "#         'Test Accuracy': test_accuracy\n",
    "#     }\n",
    "\n",
    "#     info_path = os.path.join(\n",
    "#         model_path, f'model__info.json')\n",
    "#     with open(info_path, 'w') as info_file:\n",
    "#         json.dump(info, info_file)\n",
    "\n",
    "#     # Logging: You can log the saved model path and version\n",
    "#     print(f\"Saved model: {model_path}\")\n",
    "    \n",
    "#     # Log the results in the same log file\n",
    "#     model_summary = []\n",
    "#     conv_model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#     model_architecture = \"\\n\".join(model_summary)\n",
    "#     logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\\n#################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XAI for chosen model explainaibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_seq_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "def predict_fn(texts):\n",
    "    preprocessed_texts = preprocess_text(texts)\n",
    "    return conv_model.predict(preprocessed_texts)\n",
    "\n",
    "\n",
    "# Create a LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=list(label_encoder.keys()))\n",
    "\n",
    "# Choose an instance from your dataset\n",
    "# Replace with an actual text from your dataset\n",
    "instance = \"What American composer wrote the music for `` West Side Story '' ?\"\n",
    "\n",
    "# Generate an explanation\n",
    "explanation = explainer.explain_instance(\n",
    "    instance, predict_fn, num_features=10, top_labels=3)\n",
    "\n",
    "# Show the explanation for the top class\n",
    "explanation.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# # Define a grid of hyperparameters to search\n",
    "# param_grid = {\n",
    "#     'embedding_size': [100, 300],\n",
    "#     'conv_width': [128],\n",
    "#     'lstm_units': [64],\n",
    "#     'batch_size': [64],\n",
    "#     'dense_units': [64, 256],\n",
    "#     'kernel_size': [5, 7]\n",
    "# }\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "# print('Param Combinations: ')\n",
    "# for p in param_combinations:\n",
    "#     print(p)\n",
    "\n",
    "# model_folder = 'CNN+LSTM'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# logged_model = False\n",
    "# # In the model architecture:\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     conv_width = params['conv_width']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     dense_units = params['dense_units']\n",
    "#     batch_size = params['batch_size']\n",
    "#     kernel_size = params['kernel_size']\n",
    "\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "#                                   output_dim=embedding_size,\n",
    "#                                   input_length=max_seq_length,\n",
    "#                                   trainable=False),\n",
    "#         tf.keras.layers.Conv1D(conv_width, kernel_size, activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#         tf.keras.layers.LSTM(lstm_units, return_sequences=True),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         tf.keras.layers.Dense(dense_units, activation='relu',\n",
    "#                               kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dense(5, activation='softmax')\n",
    "#     ])\n",
    "#     # Log the results in the same log file\n",
    "#     if not logged_model:\n",
    "#         model_summary = []\n",
    "#         model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#         model_architecture = \"\\n\".join(model_summary)\n",
    "#         logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#         logged_model = True\n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.01\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = model.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),\n",
    "#                         callbacks=[early_stopping])\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = model.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "\n",
    "   \n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Width={conv_width}, LSTM Units={lstm_units}, Dense Units={dense_units}, Batch Size={batch_size}\")\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\\n#####################################################################################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
