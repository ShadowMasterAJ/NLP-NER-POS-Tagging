{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Sequence Tagging: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.layers import InputLayer\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained word2vec embeddings and save the model\n",
    "# Uncomment the lines below to download and save the pretrained model\n",
    "\n",
    "# google_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "# google_vectors.save('./data/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "w2v = KeyedVectors.load('./data/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar word to student is students with a cosine similarity of 0.7294865846633911\n",
      "The most similar word to Apple is Apple_AAPL with a cosine similarity of 0.7456987500190735\n",
      "The most similar word to apple is apples with a cosine similarity of 0.720359742641449\n"
     ]
    }
   ],
   "source": [
    "#Cosine similarity\n",
    "student = w2v.most_similar('student')[0]\n",
    "Apple = w2v.most_similar('Apple')[0]\n",
    "apple = w2v.most_similar('apple')[0]\n",
    "\n",
    "print(f'The most similar word to student is {student[0]} with a cosine similarity of {student[1]}')\n",
    "print(f'The most similar word to Apple is {Apple[0]} with a cosine similarity of {Apple[1]}')\n",
    "print(f'The most similar word to apple is {apple[0]} with a cosine similarity of {apple[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to write functions that get the sentences and the tags from the train, development, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(path):\n",
    "    file_path = path\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.strip():\n",
    "                    word = line.split()[0]\n",
    "                    current_sentence.append(word)\n",
    "                else:\n",
    "                    if current_sentence:\n",
    "                        sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "            return sentences\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def getTags(path):\n",
    "    file_path = path\n",
    "    tags = []\n",
    "    current_sentence = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.strip():\n",
    "                    tag = line.split()[3]\n",
    "                    current_sentence.append(tag)\n",
    "                else:\n",
    "                    if current_sentence:\n",
    "                        tags.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            if current_sentence:\n",
    "                tags.append(current_sentence)\n",
    "            return tags\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = getSentences('./data/eng.train')\n",
    "train_tags = getTags('./data/eng.train')\n",
    "development_sentences = getSentences('./data/eng.testa')\n",
    "development_tags = getTags('./data/eng.testa')\n",
    "test_sentences = getSentences('./data/eng.testb')\n",
    "test_tags = getTags('./data/eng.testb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Describe the size of the datasets and the complete set of all possible word labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences (training): 14987\n",
      "Number of sentences (dev): 3466\n",
      "Number of sentences (test): 3684\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences (training):\", len(train_sentences))\n",
    "print(\"Number of sentences (dev):\", len(development_sentences))\n",
    "print(\"Number of sentences (test):\", len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Labels:  {'B-MISC', 'I-PER', 'I-LOC', 'I-MISC', 'O', 'B-ORG', 'B-LOC', 'I-ORG'}\n"
     ]
    }
   ],
   "source": [
    "all_tags = [tag for sentence in train_tags for tag in sentence]\n",
    "unique_tags = set(all_tags)\n",
    "print(\"Word Labels: \", unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Choose an example sentence from the Training set containing at least two named entities with more than one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            data = file.readlines()\n",
    "        file.close()\n",
    "    except Exception as e:\n",
    "        data = None\n",
    "        print(e)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data('./data/eng.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU NNP I-NP I-ORG\n",
      "rejects VBZ I-VP O\n",
      "German JJ I-NP I-MISC\n",
      "call NN I-NP O\n",
      "to TO I-VP O\n",
      "boycott VB I-VP O\n",
      "British JJ I-NP I-MISC\n",
      "lamb NN I-NP O\n",
      ". . O O\n",
      "\n",
      "EU rejects German call to boycott British lamb .\n"
     ]
    }
   ],
   "source": [
    "data_text = ''.join(train_data)\n",
    "\n",
    "sentences = data_text.strip().split('\\n\\n')\n",
    "\n",
    "for sentence in sentences:\n",
    "    lines = sentence.split('\\n')\n",
    "\n",
    "    named_entities_count = sum(1 for line in lines if (len(line.split()) > 2) \n",
    "                                                   and line.split()[3].startswith('I'))\n",
    "\n",
    "    if named_entities_count >= 2:\n",
    "        words = [line.split()[0] for line in lines]\n",
    "        print(sentence)\n",
    "        print()\n",
    "        print(' '.join(words))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the labels, you can form complete named entities as follows: \n",
    "\n",
    "EU (I-ORG): \"EU\" is the complete named entity, and the label \"I-ORG\" indicates that it is an organization. \n",
    "\n",
    "German (I-MISC): \"German\" is an incomplete named entity, and the label \"I-MISC\" indicates that it is a miscellaneous entity. \n",
    "\n",
    "British (I-MISC): \"British\" is another incomplete named entity labeled as \"I-MISC.\" \n",
    "\n",
    "So, in this sentence, \"EU\" is a complete named entity, and \"German\" and \"British\" are incomplete named entities. The specific nature of the incomplete entities is not specified in this sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/eng.train'\n",
    "development_path = './data/eng.testa'\n",
    "test_path = './data/eng.testb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_path)\n",
    "development_data = get_data(development_path)\n",
    "test_data = get_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(data):\n",
    "    split_data = [line.split(' ') for line in data] if data != None else []\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    for line in split_data:\n",
    "        if line == ['\\n']:\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "        else:\n",
    "            word = line[0]\n",
    "            tag = line[-1].replace('\\n', '')\n",
    "            current_sentence.append([word, tag])\n",
    "    sentences.append(current_sentence)\n",
    "    return sentences\n",
    "\n",
    "def extract_words_and_tags(data):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for line in data:\n",
    "        parts = line.split()\n",
    "        if parts:\n",
    "            word = parts[0]\n",
    "            tag = parts[-1]\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = extract_sentences(train_data)\n",
    "train_words, train_tags = extract_words_and_tags(train_data)\n",
    "development_sentences = extract_sentences(development_data)\n",
    "development_words, development_tags = extract_words_and_tags(development_data)\n",
    "test_sentences = extract_sentences(test_data)\n",
    "test_words, test_tags = extract_words_and_tags(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_voc = np.unique(np.array(train_words))\n",
    "dev_voc = np.unique(np.array(development_words))\n",
    "tag_set = np.unique(np.array(train_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pretrained_weights = w2v.vectors\n",
    "train_num_tokens, train_embedding_dim = train_pretrained_weights.shape\n",
    "\n",
    "word2idx = w2v.key_to_index\n",
    "word2idx['<UNK>'] = word2idx[list(word2idx.keys())[-1]]+1\n",
    "word2idx['<PAD>'] = word2idx[list(word2idx.keys())[-1]]+1\n",
    "voc = word2idx.keys()\n",
    "# voc = np.append(voc,'<UNK>')\n",
    "# voc = np.append(voc,'<PAD>')\n",
    "\n",
    "tag2idx = {k: v for v, k in enumerate(tag_set)}\n",
    "tag2idx['<PAD>'] = 8\n",
    "index_to_label={v:k for k,v in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(voc)\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_embeddings(sentences):\n",
    "  sequence = []\n",
    "  sent_seq = []\n",
    "  for s in sentences:\n",
    "    for w in s:\n",
    "      if w[0] in word2idx.keys():\n",
    "        sent_seq.append(word2idx[w[0]])\n",
    "      else:\n",
    "        sent_seq.append(word2idx['<UNK>'])\n",
    "    sequence.append(sent_seq)\n",
    "    sent_seq = []\n",
    "  return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "undersample = CondensedNearestNeighbour(n_neighbors=1)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "x_train = get_x_embeddings(train_sentences)\n",
    "y_train = [[tag2idx[w[1]] for w in s] for s in train_sentences]\n",
    "x_dev = get_x_embeddings(development_sentences)\n",
    "y_dev = [[tag2idx[w[1]] for w in s] for s in development_sentences]\n",
    "x_test = get_x_embeddings(test_sentences)\n",
    "y_test = [[tag2idx[w[1]] for w in s] for s in test_sentences]\n",
    "\n",
    "x_train = pad_sequences(maxlen=embedding_dim, sequences=x_train, padding=\"post\")\n",
    "y_train = pad_sequences(maxlen=embedding_dim, sequences=y_train, padding=\"post\")\n",
    "x_dev = pad_sequences(maxlen=embedding_dim, sequences=x_dev, padding=\"post\")\n",
    "y_dev = pad_sequences(maxlen=embedding_dim, sequences=y_dev, padding=\"post\")\n",
    "x_test = pad_sequences(maxlen=embedding_dim, sequences=x_test, padding=\"post\")\n",
    "y_test = pad_sequences(maxlen=embedding_dim, sequences=y_test, padding=\"post\")\n",
    "\n",
    "y_train_1D=y_train.reshape(-1)\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(y_train_1D),\n",
    "                                        y =y_train_1D                                           \n",
    "                                    )\n",
    "class_weights = dict(zip(np.unique(y_train_1D), class_weights))\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_dev = to_categorical(y_dev)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 50, 50)            150000100 \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 256)               183296    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 400)               205200    \n",
      "                                                                 \n",
      " reshape_11 (Reshape)        (None, 50, 8)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 150521204 (574.19 MB)\n",
      "Trainable params: 520592 (1.99 MB)\n",
      "Non-trainable params: 150000612 (572.21 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout,BatchNormalization,Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from seqeval.metrics import f1_score as seq_f1_score\n",
    "from keras.callbacks import Callback\n",
    "from seqeval.scheme import IOB1\n",
    "\n",
    "num_classes=8\n",
    "sequence_length=50\n",
    "output_shape=(sequence_length,num_classes)\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data,index_to_label):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.index_to_label=index_to_label\n",
    "        self.best_f1 = 0\n",
    "        self.wait = 0\n",
    "        self.patience = 10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        \n",
    "        y_true = self.convert_to_labels(y_val)\n",
    "        y_pred = self.convert_to_labels(y_pred)\n",
    "\n",
    "        f1 = seq_f1_score(y_true, y_pred, scheme=IOB1)\n",
    "        print(f' - F1 Score: {f1}')\n",
    "        \n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "\n",
    "        if self.wait >= self.patience:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def convert_to_labels(self, data):\n",
    "        labels = []\n",
    "        for seq in data:\n",
    "            label_seq = [self.index_to_label[np.argmax(token)] for token in seq]\n",
    "            labels.append(label_seq)\n",
    "        return labels\n",
    "\n",
    "   \n",
    "# Define and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_tokens, output_dim=embedding_dim, input_length=sequence_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=512, activation='relu', kernel_regularizer=regularizers.L1L2(l1=0.25, l2=0.25)))\n",
    "model.add(Dense(num_classes*sequence_length, activation='softmax'))\n",
    "model.add(tf.keras.layers.Reshape(output_shape))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "f1_callback = F1ScoreCallback(validation_data=(x_dev, y_dev),index_to_label=index_to_label)\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 2s 17ms/step\n",
      " - F1 Score: 0.8194334725200315\n",
      "15/15 [==============================] - 24s 1s/step - loss: 539.7273 - accuracy: 0.6961 - val_loss: 217.1054 - val_accuracy: 0.7083 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 2s 16ms/step\n",
      " - F1 Score: 0.832748189552729\n",
      "15/15 [==============================] - 14s 959ms/step - loss: 136.4453 - accuracy: 0.7807 - val_loss: 87.1042 - val_accuracy: 0.7196 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "109/109 [==============================] - 2s 17ms/step\n",
      " - F1 Score: 0.8439918561764818\n",
      "15/15 [==============================] - 14s 959ms/step - loss: 68.7753 - accuracy: 0.7970 - val_loss: 52.3257 - val_accuracy: 0.7421 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "109/109 [==============================] - 2s 15ms/step\n",
      " - F1 Score: 0.8460465932915852\n",
      "15/15 [==============================] - 14s 943ms/step - loss: 49.5757 - accuracy: 0.8086 - val_loss: 46.3998 - val_accuracy: 0.7450 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "109/109 [==============================] - 2s 16ms/step\n",
      " - F1 Score: 0.8505142344010888\n",
      "15/15 [==============================] - 14s 955ms/step - loss: 45.4768 - accuracy: 0.8258 - val_loss: 43.7894 - val_accuracy: 0.7548 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "109/109 [==============================] - 2s 16ms/step\n",
      " - F1 Score: 0.8485974544720708\n",
      "15/15 [==============================] - 15s 1s/step - loss: 44.1265 - accuracy: 0.8346 - val_loss: 43.3432 - val_accuracy: 0.7524 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "109/109 [==============================] - 2s 17ms/step\n",
      " - F1 Score: 0.8599688064069537\n",
      "15/15 [==============================] - 15s 1s/step - loss: 43.9182 - accuracy: 0.8399 - val_loss: 42.9336 - val_accuracy: 0.7674 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "109/109 [==============================] - 2s 16ms/step\n",
      " - F1 Score: 0.8194334725200315\n",
      "15/15 [==============================] - 15s 1s/step - loss: 44.6816 - accuracy: 0.8130 - val_loss: 45.1269 - val_accuracy: 0.7083 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "109/109 [==============================] - 2s 18ms/step\n",
      " - F1 Score: 0.8267953322872531\n",
      "15/15 [==============================] - 15s 1s/step - loss: 45.3270 - accuracy: 0.7674 - val_loss: 43.5773 - val_accuracy: 0.7085 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "109/109 [==============================] - 2s 17ms/step\n",
      " - F1 Score: 0.8457983402077224\n",
      "15/15 [==============================] - 16s 1s/step - loss: 43.3407 - accuracy: 0.8211 - val_loss: 41.7779 - val_accuracy: 0.7495 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "109/109 [==============================] - 2s 19ms/step\n",
      " - F1 Score: 0.8443338169466351\n",
      "15/15 [==============================] - 16s 1s/step - loss: 42.4904 - accuracy: 0.8256 - val_loss: 41.5784 - val_accuracy: 0.7408 - lr: 0.0100\n",
      "Epoch 12/20\n",
      "109/109 [==============================] - 2s 19ms/step\n",
      " - F1 Score: 0.8535653709752029\n",
      "15/15 [==============================] - 17s 1s/step - loss: 42.0818 - accuracy: 0.8407 - val_loss: 40.7910 - val_accuracy: 0.7576 - lr: 0.0100\n",
      "Epoch 13/20\n",
      "109/109 [==============================] - 2s 19ms/step\n",
      " - F1 Score: 0.850461580936276\n",
      "15/15 [==============================] - 17s 1s/step - loss: 40.8923 - accuracy: 0.8438 - val_loss: 39.9560 - val_accuracy: 0.7516 - lr: 0.0100\n",
      "Epoch 14/20\n",
      "109/109 [==============================] - 2s 21ms/step\n",
      " - F1 Score: 0.8307982874157317\n",
      "15/15 [==============================] - 18s 1s/step - loss: 40.5702 - accuracy: 0.8434 - val_loss: 40.5266 - val_accuracy: 0.7210 - lr: 0.0100\n",
      "Epoch 15/20\n",
      "109/109 [==============================] - 2s 20ms/step\n",
      " - F1 Score: 0.8457047825015741\n",
      "15/15 [==============================] - 21s 1s/step - loss: 40.2648 - accuracy: 0.8481 - val_loss: 39.4604 - val_accuracy: 0.7424 - lr: 0.0100\n",
      "Epoch 16/20\n",
      "109/109 [==============================] - 2s 15ms/step\n",
      " - F1 Score: 0.8433904979587341\n",
      "15/15 [==============================] - 15s 999ms/step - loss: 40.1451 - accuracy: 0.8454 - val_loss: 39.5155 - val_accuracy: 0.7426 - lr: 0.0100\n",
      "Epoch 17/20\n",
      "109/109 [==============================] - 2s 16ms/step\n",
      " - F1 Score: 0.8500047066370555\n",
      "15/15 [==============================] - 18s 1s/step - loss: 39.7257 - accuracy: 0.8484 - val_loss: 38.9918 - val_accuracy: 0.7489 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1de5ab35850>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 1024\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=num_epochs, validation_data=(x_dev, y_dev), callbacks=[early_stopping,f1_callback, lr_scheduler], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 2s 13ms/step\n",
      "(3684, 50, 8)\n",
      "(3684, 50, 8)\n",
      "F1 Score: 0.8734398715520376\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# print(f' - F1 Score: {f1}')\n",
    "predictions = model.predict(x_test)\n",
    "print(predictions.shape)\n",
    "print(y_test.shape)\n",
    "predicted_label_sequences = []  # List to store predicted label sequences\n",
    "true_label_sequences = []  # List to store true label sequences\n",
    "for sequence in predictions:\n",
    "    # Decode the predicted labels for each sequence\n",
    "    decoded_sequence = [index_to_label[np.argmax(label,axis=-1)] for label in sequence]\n",
    "    predicted_label_sequences.append(decoded_sequence)\n",
    "for sequence in y_test:\n",
    "    # Decode the true labels for each sequence\n",
    "    decoded_sequence = [index_to_label[np.argmax(label,axis=-1)] for label in sequence]\n",
    "    true_label_sequences.append(decoded_sequence)\n",
    "f1 = f1_score(true_label_sequences, predicted_label_sequences, scheme=IOB1)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
