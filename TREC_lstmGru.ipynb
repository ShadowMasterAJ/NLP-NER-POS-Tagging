{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 18:53:11.267366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size:  4952\n",
      "Development Size:  500\n",
      "Test Size:  500\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv('TREC_dataset/dev.csv')\n",
    "train_df = pd.read_csv('TREC_dataset/train.csv')\n",
    "test_df = pd.read_csv('TREC_dataset/test.csv')\n",
    "print(\"Training Size: \", train_df.shape[0])\n",
    "print(\"Development Size: \", dev_df.shape[0])\n",
    "print(\"Test Size: \", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/pretrained/word2vec_model.gensim'\n",
    "w2v_model = KeyedVectors.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  3000000\n",
      "Embedding Size:  300\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = w2v_model.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "\n",
    "print(\"Vocab Size: \", vocab_size)\n",
    "print(\"Embedding Size: \", embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.word_index = {e: i for e,\n",
    "                        i in w2v_model.key_to_index.items()}\n",
    "\n",
    "# Convert text data to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad sequences to ensure they have the same length\n",
    "max_seq_length = 50  # Adjust as needed\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
    "dev_data = pad_sequences(dev_sequences, maxlen=max_seq_length, padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = {label: i for i, label in enumerate(train_df['label-coarse'].unique())}\n",
    "\n",
    "train_labels = np.array([label_encoder[label] for label in train_df['label-coarse']])\n",
    "dev_labels = np.array([label_encoder[label] for label in dev_df['label-coarse']])\n",
    "test_labels = np.array([label_encoder[label] for label in test_df['label-coarse']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OTHERS': 0, '1': 1, '2': 2, '4': 3, '5': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: Embedding Size=200, LSTM Units=64, Batch Size=512\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 23s 2s/step - loss: 2.2095 - accuracy: 0.3677 - val_loss: 1.7229 - val_accuracy: 0.4120\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 9s 920ms/step - loss: 1.1575 - accuracy: 0.6056 - val_loss: 1.5764 - val_accuracy: 0.4120\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 9s 926ms/step - loss: 0.8586 - accuracy: 0.7143 - val_loss: 1.4435 - val_accuracy: 0.4120\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 8s 824ms/step - loss: 0.7110 - accuracy: 0.7450 - val_loss: 1.3942 - val_accuracy: 0.4120\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.5984 - accuracy: 0.7829 - val_loss: 1.3750 - val_accuracy: 0.4120\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 3s 282ms/step - loss: 0.5362 - accuracy: 0.8112 - val_loss: 1.3888 - val_accuracy: 0.4120\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 10s 1s/step - loss: 0.4952 - accuracy: 0.8233 - val_loss: 1.3722 - val_accuracy: 0.4120\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 6s 662ms/step - loss: 0.4511 - accuracy: 0.8437 - val_loss: 1.3102 - val_accuracy: 0.4120\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 3s 252ms/step - loss: 0.4276 - accuracy: 0.8451 - val_loss: 1.3114 - val_accuracy: 0.4120\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 3s 258ms/step - loss: 0.3800 - accuracy: 0.8695 - val_loss: 1.3532 - val_accuracy: 0.4120\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.3543 - accuracy: 0.8776 - val_loss: 1.2337 - val_accuracy: 0.4220\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 3s 325ms/step - loss: 0.3310 - accuracy: 0.8889 - val_loss: 1.2380 - val_accuracy: 0.4220\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 3s 267ms/step - loss: 0.3039 - accuracy: 0.8980 - val_loss: 1.2771 - val_accuracy: 0.4180\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 3s 260ms/step - loss: 0.2672 - accuracy: 0.9107 - val_loss: 1.2351 - val_accuracy: 0.4200\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 9s 993ms/step - loss: 0.2431 - accuracy: 0.9259 - val_loss: 1.2005 - val_accuracy: 0.4300\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 3s 254ms/step - loss: 0.2315 - accuracy: 0.9253 - val_loss: 1.3124 - val_accuracy: 0.4220\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 9s 919ms/step - loss: 0.1991 - accuracy: 0.9378 - val_loss: 1.0895 - val_accuracy: 0.4600\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 6s 658ms/step - loss: 0.1630 - accuracy: 0.9501 - val_loss: 1.0599 - val_accuracy: 0.4880\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 8s 888ms/step - loss: 0.1534 - accuracy: 0.9542 - val_loss: 0.9929 - val_accuracy: 0.5500\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 3s 259ms/step - loss: 0.1345 - accuracy: 0.9655 - val_loss: 1.0745 - val_accuracy: 0.5220\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 9s 1s/step - loss: 0.1343 - accuracy: 0.9634 - val_loss: 0.8927 - val_accuracy: 0.6440\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 3s 270ms/step - loss: 0.1186 - accuracy: 0.9673 - val_loss: 1.0463 - val_accuracy: 0.5880\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 3s 260ms/step - loss: 0.1292 - accuracy: 0.9651 - val_loss: 0.9668 - val_accuracy: 0.6060\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.1063 - accuracy: 0.9739 - val_loss: 0.9664 - val_accuracy: 0.6160\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 11s 1s/step - loss: 0.1107 - accuracy: 0.9717 - val_loss: 0.7227 - val_accuracy: 0.7520\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 3s 259ms/step - loss: 0.0940 - accuracy: 0.9798 - val_loss: 0.7923 - val_accuracy: 0.7220\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 8s 855ms/step - loss: 0.0830 - accuracy: 0.9800 - val_loss: 0.6829 - val_accuracy: 0.7820\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.0750 - accuracy: 0.9822 - val_loss: 0.7926 - val_accuracy: 0.7060\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 3s 262ms/step - loss: 0.0680 - accuracy: 0.9853 - val_loss: 0.8261 - val_accuracy: 0.7140\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 3s 273ms/step - loss: 0.0789 - accuracy: 0.9806 - val_loss: 0.7893 - val_accuracy: 0.7320\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 3s 260ms/step - loss: 0.0831 - accuracy: 0.9810 - val_loss: 0.8087 - val_accuracy: 0.7480\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 3s 262ms/step - loss: 0.0799 - accuracy: 0.9804 - val_loss: 0.8080 - val_accuracy: 0.7640\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 3s 274ms/step - loss: 0.0744 - accuracy: 0.9859 - val_loss: 0.7579 - val_accuracy: 0.7740\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.0868 - accuracy: 0.9796 - val_loss: 0.8174 - val_accuracy: 0.7640\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 3s 264ms/step - loss: 0.0778 - accuracy: 0.9810 - val_loss: 0.8534 - val_accuracy: 0.7720\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.0659 - accuracy: 0.9867 - val_loss: 0.6943 - val_accuracy: 0.8020\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 6s 676ms/step - loss: 0.0552 - accuracy: 0.9889 - val_loss: 0.7424 - val_accuracy: 0.7800\n",
      "16/16 - 0s - loss: 0.7512 - accuracy: 0.7500 - 193ms/epoch - 12ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Create a directory for model logs\n",
    "if not os.path.exists('model_logs'):\n",
    "    os.mkdir('model_logs')\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'embedding_size': [200],# Different embedding sizes\n",
    "    'lstm_units': [64],        # Different LSTM units\n",
    "    'batch_size': [512],         # Different batch sizes\n",
    "}\n",
    "\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "model_folder = 'BiLSTM'\n",
    "\n",
    "# Create a directory for the current model\n",
    "model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "if not os.path.exists(model_logs_dir):\n",
    "    os.mkdir(model_logs_dir)\n",
    "\n",
    "# Configure logging to save results to a single log file\n",
    "log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "logging.basicConfig(filename=log_filepath,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Iterate through the parameter combinations\n",
    "for params in param_combinations:\n",
    "    embedding_size = params['embedding_size']\n",
    "    lstm_units = params['lstm_units']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    print(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    \n",
    "    from tensorflow.keras.layers import Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Embedding, BatchNormalization\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "\n",
    "    lstm_gru_cnn = tf.keras.Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_seq_length, trainable=False),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        Conv1D(128, 5, activation='relu', padding='same'),  # Convolutional layer\n",
    "        GlobalMaxPooling1D(),  # Global Max Pooling\n",
    "        Dropout(0.2),  # Dropout for regularization\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # Adding L2 regularization\n",
    "        BatchNormalization(),  # Batch normalization layer Adding another layer with L2 regularization\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "    initial_learning_rate = 0.01\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    lstm_gru_cnn.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define EarlyStopping callback to prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    class_weights = {0: 0.454, 1: 0.869, 2: 13.042, 3: 1.228, 4: 1.312}\n",
    "    # Train the model\n",
    "    history = lstm_gru_cnn.fit(train_data, train_labels, epochs=50,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(dev_data, dev_labels),\n",
    "                        callbacks=[early_stopping],\n",
    "                        workers=10\n",
    "                        # class_weight=class_weights\n",
    "                        )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = lstm_gru_cnn.evaluate(\n",
    "        test_data, test_labels, verbose=2)\n",
    "\n",
    "    # Log the results in the same log file\n",
    "    model_summary = []\n",
    "    lstm_gru_cnn.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_architecture = \"\\n\".join(model_summary)\n",
    "    logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "    logging.info(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    logging.info(f\"Test Loss: {test_loss}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(Attention, self).__init__()\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W_q = self.add_weight(\"W_q\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "#         self.W_k = self.add_weight(\"W_k\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "#         self.W_v = self.add_weight(\"W_v\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         Q = tf.matmul(inputs, self.W_q)\n",
    "#         K = tf.matmul(inputs, self.W_k)\n",
    "#         V = tf.matmul(inputs, self.W_v)\n",
    "\n",
    "#         attention_scores = tf.matmul(Q, K, transpose_b=True)\n",
    "#         attention_scores = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "#         output = tf.matmul(attention_scores, V)\n",
    "#         return output\n",
    "\n",
    "# # Define a grid of hyperparameters to search\n",
    "# param_grid = {\n",
    "#     'embedding_size': [150],  # Different embedding sizes\n",
    "#     'lstm_units': [64],      # Different LSTM units\n",
    "#     'gru_units': [32],        # Different GRU units\n",
    "#     'batch_size': [64],      # Different batch sizes\n",
    "# }\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "# print('Param Combinations: ',param_combinations)\n",
    "# model_folder = 'Ensemble_Model'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Iterate through the parameter combinations\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     gru_units = params['gru_units']\n",
    "#     batch_size = params['batch_size']\n",
    "\n",
    "    \n",
    "#     print(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, GRU Units={gru_units}, Batch Size={batch_size}\")\n",
    "\n",
    "#     # Define your model with attention\n",
    "#     # Define your simpler model architecture\n",
    "#     attention_LGRB = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(\n",
    "#             input_dim=vocab_size,\n",
    "#             output_dim=embedding_size,\n",
    "#             input_length=max_seq_length,\n",
    "#             trainable=False\n",
    "#         ),\n",
    "#         # Dropout for the embedding layer\n",
    "#         tf.keras.layers.SpatialDropout1D(0.2),\n",
    "#         tf.keras.layers.LSTM(lstm_units, return_sequences=True,\n",
    "#                              kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # L2 regularization\n",
    "#         tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "#         tf.keras.layers.GRU(gru_units, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "#         Attention(),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(\n",
    "#             0.001)),  # L2 regularization\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(5, activation='softmax')\n",
    "#     ])\n",
    "\n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.1\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "#     attention_LGRB.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = attention_LGRB.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),)\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = attention_LGRB.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "\n",
    "#     # Log the results in the same log file\n",
    "#     model_summary = []\n",
    "#     attention_LGRB.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#     model_architecture = \"\\n\".join(model_summary)\n",
    "#     logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, GRU Units={gru_units}, Batch Size={batch_size}\")\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features):\n",
    "        query_value_attention_score = self.V(tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(features)))\n",
    "        attention_weights = tf.nn.softmax(query_value_attention_score, axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Updated hyperparameter grid to include depth and width\n",
    "param_grid = {\n",
    "    'embedding_size': [100],     # Embedding sizes\n",
    "    'conv_depth': [1],          # Number of Conv1D layers (Depth)\n",
    "    'conv_width': [128],    # Number of filters in each Conv1D layer (Width)\n",
    "    'kernel_size': [5],         # Kernel sizes\n",
    "    'batch_size': [64],      # Batch sizes\n",
    "    'attention_units': [8]\n",
    "}\n",
    "\n",
    "\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "print('Param Combinations:')\n",
    "for p in param_combinations:\n",
    "    print(p)\n",
    "\n",
    "model_folder = 'Ensemble_Model'\n",
    "\n",
    "# Create a directory for the current model\n",
    "model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "if not os.path.exists(model_logs_dir):\n",
    "    os.mkdir(model_logs_dir)\n",
    "\n",
    "# Configure logging to save results to a single log file\n",
    "log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "logging.basicConfig(filename=log_filepath,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# In the model architecture:\n",
    "for params in param_combinations:\n",
    "    embedding_size = params['embedding_size']\n",
    "    conv_depth = params['conv_depth']\n",
    "    conv_width = params['conv_width']\n",
    "    kernel_size = params['kernel_size']\n",
    "    batch_size = params['batch_size']\n",
    "    attention_units = params['attention_units']\n",
    "    # Log the current hyperparameters\n",
    "    print(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Depth={conv_depth}, Conv Width={conv_width}, Kernel Size={kernel_size}, Batch Size={batch_size}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    input_layer = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                output_dim=embedding_size,\n",
    "                                input_length=max_seq_length,\n",
    "                                trainable=False)(input_layer)\n",
    "    conv_layer = tf.keras.layers.Conv1D(conv_width, 5, activation='relu')(embedding_layer)\n",
    "    context_vector, attention_weights = Attention(attention_units)(conv_layer)\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(context_vector)\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.6)(dense_layer)\n",
    "    output_layer = tf.keras.layers.Dense(5, activation='softmax')(dropout_layer)\n",
    "\n",
    "    conv_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "    initial_learning_rate = 0.01\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "   \n",
    "    conv_model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define EarlyStopping callback to prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = conv_model.fit(train_data, train_labels, epochs=50,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(dev_data, dev_labels),\n",
    "                        callbacks=[early_stopping],\n",
    "                        workers=8,\n",
    "                        )\n",
    "\n",
    "    # Evaluate the model on the test se\n",
    "    test_loss, test_accuracy = conv_model.evaluate(\n",
    "        test_data, test_labels, verbose=2)\n",
    "    logging.info(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Depth={conv_depth}, Conv Width={conv_width}, Kernel Size={kernel_size}, Batch Size={batch_size}\")\n",
    "    \n",
    "    # Log the results in the same log file\n",
    "    model_summary = []\n",
    "    conv_model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_architecture = \"\\n\".join(model_summary)\n",
    "    logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "    logging.info(f\"Test Loss: {test_loss}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy}\\n#################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming your tokenizer and label_encoder are defined as per your provided code\n",
    "\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_seq_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "def predict_fn(texts):\n",
    "    preprocessed_texts = preprocess_text(texts)\n",
    "    return conv_model.predict(preprocessed_texts)\n",
    "\n",
    "\n",
    "# Create a LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=list(label_encoder.keys()))\n",
    "\n",
    "# Choose an instance from your dataset\n",
    "# Replace with an actual text from your dataset\n",
    "instance = \"What American composer wrote the music for `` West Side Story '' ?\"\n",
    "\n",
    "# Generate an explanation\n",
    "explanation = explainer.explain_instance(\n",
    "    instance, predict_fn, num_features=10, top_labels=3)\n",
    "\n",
    "# Show the explanation for the top class\n",
    "explanation.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# # Define a grid of hyperparameters to search\n",
    "# param_grid = {\n",
    "#     'embedding_size': [100, 300],\n",
    "#     'conv_width': [128],\n",
    "#     'lstm_units': [64],\n",
    "#     'batch_size': [64],\n",
    "#     'dense_units': [64, 256],\n",
    "#     'kernel_size': [5, 7]\n",
    "# }\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "# print('Param Combinations: ')\n",
    "# for p in param_combinations:\n",
    "#     print(p)\n",
    "\n",
    "# model_folder = 'CNN+LSTM'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# logged_model = False\n",
    "# # In the model architecture:\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     conv_width = params['conv_width']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     dense_units = params['dense_units']\n",
    "#     batch_size = params['batch_size']\n",
    "#     kernel_size = params['kernel_size']\n",
    "\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "#                                   output_dim=embedding_size,\n",
    "#                                   input_length=max_seq_length,\n",
    "#                                   trainable=False),\n",
    "#         tf.keras.layers.Conv1D(conv_width, kernel_size, activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#         tf.keras.layers.LSTM(lstm_units, return_sequences=True),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         tf.keras.layers.Dense(dense_units, activation='relu',\n",
    "#                               kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dense(5, activation='softmax')\n",
    "#     ])\n",
    "#     # Log the results in the same log file\n",
    "#     if not logged_model:\n",
    "#         model_summary = []\n",
    "#         model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#         model_architecture = \"\\n\".join(model_summary)\n",
    "#         logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#         logged_model = True\n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.01\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = model.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),\n",
    "#                         callbacks=[early_stopping])\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = model.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "\n",
    "   \n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Width={conv_width}, LSTM Units={lstm_units}, Dense Units={dense_units}, Batch Size={batch_size}\")\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\\n#####################################################################################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
