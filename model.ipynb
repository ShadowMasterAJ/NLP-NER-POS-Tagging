{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 18:49:57.024750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "1. Load the embedding matrix and tokenizer. Tokenise the data using the tokeniser\n",
    "\n",
    "2. Convert text data from three different data frames (train_df, dev_df, and test_df) into sequences of integers. Each word in the text is replaced by its corresponding index from the word index created earlier.\n",
    "\n",
    "3. Set a maximum sequence length (max_seq_length) and then pad the sequences to ensure that they all have the same length. \\\n",
    "    a. Padding is done with zeros (`0.`)\n",
    "\n",
    "4. Create a label encoder by mapping unique labels in the 'label-coarse' column of the training data frame to integers.\n",
    "\n",
    "5. Encode the labels for the training, development, and test data sets using this label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reduced embedding matrix\n",
    "embedding_matrix = np.load('models/embedding_matrix.npy')\n",
    "vocab_size, embedding_size = embedding_matrix.shape\n",
    "\n",
    "# Load saved tokenizer\n",
    "with open('models/tokenizer.json') as f:\n",
    "    tokenizer_data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_data)\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_csv('TREC_dataset/train.csv')\n",
    "dev_df = pd.read_csv('TREC_dataset/dev.csv')\n",
    "test_df = pd.read_csv('TREC_dataset/test.csv')\n",
    "\n",
    "# Convert text data to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad sequences to ensure they have the same length\n",
    "max_seq_length = 40  # Adjust as needed\n",
    "\n",
    "train_data = pad_sequences(\n",
    "    train_sequences, maxlen=max_seq_length, padding='post')\n",
    "dev_data = pad_sequences(dev_sequences, maxlen=max_seq_length, padding='post')\n",
    "test_data = pad_sequences(\n",
    "    test_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = {label: i for i, label in enumerate(train_df['label-coarse'].unique())}\n",
    "\n",
    "train_labels = np.array([label_encoder[label]for label in train_df['label-coarse']])\n",
    "dev_labels = np.array([label_encoder[label]for label in dev_df['label-coarse']])\n",
    "test_labels = np.array([label_encoder[label]for label in test_df['label-coarse']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM + CNN [FINALISED]\n",
    "\n",
    "### Step 1: Define Hyperparameter Grid\n",
    "\n",
    "1. Define a grid of hyperparameters to search, including:\n",
    "   - `embedding_size`: Different embedding sizes.\n",
    "   - `lstm_units`: Different LSTM units.\n",
    "   - `batch_size`: Different batch sizes.\n",
    "\n",
    "### Step 2: Logging and Model Directory Setup\n",
    "\n",
    "2. Create a directory structure to store model logs.\n",
    "3. Configure logging to track and save results to a log file.\n",
    "\n",
    "### Step 3: Model Training Loop\n",
    "\n",
    "In this step, we systematically train multiple models, each with different hyperparameter settings, to identify the best-performing configuration. The process is as follows:\n",
    "\n",
    "#### Substep 1: Hyperparameter Iteration\n",
    "\n",
    "4.1. Iterate through the predefined hyperparameter combinations.\n",
    "   - For each combination, we explore various settings for:\n",
    "     - `embedding_size`: The size of word embeddings.\n",
    "     - `lstm_units`: The number of LSTM units in the bidirectional LSTM layer.\n",
    "     - `batch_size`: The batch size used during training.\n",
    "\n",
    "#### Substep 2: Model Architecture\n",
    "\n",
    "4.2. Build the model architecture for the current hyperparameter combination. The architecture includes:\n",
    "   - Embedding Layer: Converts input sequences into dense vector representations.\n",
    "   - Bidirectional LSTM: A recurrent layer that captures contextual information bidirectionally.\n",
    "   - Convolutional Layer: Applies convolutional operations to capture local patterns.\n",
    "   - Global Max Pooling: Extracts the most relevant information from convolutional outputs.\n",
    "   - Dropout: Regularization technique to prevent overfitting.\n",
    "   - Dense Layers: Fully connected layers for classification.\n",
    "   \n",
    "#### Substep 3: Model Compilation\n",
    "\n",
    "4.3. Compile the model with the following configurations:\n",
    "   - Learning Rate Schedule: Uses an exponential decay schedule to adjust the learning rate.\n",
    "   - Optimizer: Utilizes the Adam optimizer for gradient descent.\n",
    "   \n",
    "#### Substep 4: Early Stopping\n",
    "\n",
    "4.4. Implement early stopping as a precautionary measure to prevent overfitting during training. Early stopping monitors the loss on the training set and stops training if the loss on the training set does not improve for a specified number of epochs.\n",
    "\n",
    "#### Substep 5: Class Weights\n",
    "\n",
    "4.5. Define class weights to address data imbalance issues. Class weights assign higher importance to underrepresented classes during training, helping the model better learn from imbalanced data.\n",
    "\n",
    "#### Substep 6: Training and Validation\n",
    "\n",
    "4.6. Train the model on the training dataset with the specified hyperparameters. During training, the model learns to make predictions based on input sequences. Validation is performed on a separate development dataset to assess the model's performance during training.\n",
    "\n",
    "By systematically exploring different hyperparameter combinations and training models with varying configurations, we aim to identify the best-performing model with the most suitable hyperparameters for the text classification task.\n",
    "\n",
    "\n",
    "### Step 4: Model Saving\n",
    "\n",
    "5. After training, evaluate the model on the test set.\n",
    "6. Save the model in a directory named based on its test accuracy (rounded to four decimal places).\n",
    "7. Record additional information in a JSON file, including the model summary.\n",
    "8. Log the saved model path and version.\n",
    "\n",
    "Summary\n",
    "\n",
    "This code demonstrates a systematic approach to hyperparameter tuning and model saving for text classification tasks, ensuring reproducibility and easy tracking of model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: Embedding Size=300, LSTM Units=32, Batch Size=128\n",
      "Epoch 1/50\n",
      "39/39 [==============================] - 6s 67ms/step - loss: 1.5287 - accuracy: 0.5396 - val_loss: 1.5750 - val_accuracy: 0.4120\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.7720 - accuracy: 0.7215 - val_loss: 1.3424 - val_accuracy: 0.4120\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.5909 - accuracy: 0.7835 - val_loss: 1.3133 - val_accuracy: 0.4120\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.5030 - accuracy: 0.8217 - val_loss: 1.1928 - val_accuracy: 0.4600\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.4543 - accuracy: 0.8407 - val_loss: 1.0594 - val_accuracy: 0.5260\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.3957 - accuracy: 0.8639 - val_loss: 0.7541 - val_accuracy: 0.7120\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 3s 75ms/step - loss: 0.3224 - accuracy: 0.8934 - val_loss: 0.7920 - val_accuracy: 0.6880\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 3s 78ms/step - loss: 0.3171 - accuracy: 0.8922 - val_loss: 0.6390 - val_accuracy: 0.7620\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.2470 - accuracy: 0.9255 - val_loss: 0.6260 - val_accuracy: 0.7820\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 3s 73ms/step - loss: 0.2092 - accuracy: 0.9346 - val_loss: 0.8600 - val_accuracy: 0.7400\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.1854 - accuracy: 0.9477 - val_loss: 0.6599 - val_accuracy: 0.8120\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.1376 - accuracy: 0.9634 - val_loss: 0.8925 - val_accuracy: 0.7840\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.1301 - accuracy: 0.9649 - val_loss: 1.0999 - val_accuracy: 0.7900\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.1422 - accuracy: 0.9622 - val_loss: 0.9877 - val_accuracy: 0.8100\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.1225 - accuracy: 0.9675 - val_loss: 0.9352 - val_accuracy: 0.8200\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0955 - accuracy: 0.9782 - val_loss: 0.8605 - val_accuracy: 0.8080\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0984 - accuracy: 0.9764 - val_loss: 0.9268 - val_accuracy: 0.8300\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.0690 - accuracy: 0.9859 - val_loss: 0.8690 - val_accuracy: 0.8220\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0641 - accuracy: 0.9871 - val_loss: 1.1261 - val_accuracy: 0.8180\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0699 - accuracy: 0.9867 - val_loss: 1.2714 - val_accuracy: 0.8060\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0586 - accuracy: 0.9907 - val_loss: 1.1935 - val_accuracy: 0.8000\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0647 - accuracy: 0.9887 - val_loss: 1.2937 - val_accuracy: 0.8100\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0734 - accuracy: 0.9861 - val_loss: 1.2990 - val_accuracy: 0.7860\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0585 - accuracy: 0.9883 - val_loss: 1.2167 - val_accuracy: 0.8160\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0661 - accuracy: 0.9859 - val_loss: 1.3571 - val_accuracy: 0.8240\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0521 - accuracy: 0.9905 - val_loss: 1.1043 - val_accuracy: 0.8280\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0370 - accuracy: 0.9952 - val_loss: 1.0485 - val_accuracy: 0.8180\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0294 - accuracy: 0.9964 - val_loss: 1.0835 - val_accuracy: 0.8200\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0328 - accuracy: 0.9956 - val_loss: 1.2458 - val_accuracy: 0.8080\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0272 - accuracy: 0.9966 - val_loss: 1.0947 - val_accuracy: 0.8300\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0277 - accuracy: 0.9980 - val_loss: 1.1762 - val_accuracy: 0.8320\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0228 - accuracy: 0.9970 - val_loss: 1.4194 - val_accuracy: 0.8160\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0454 - accuracy: 0.9943 - val_loss: 1.4240 - val_accuracy: 0.8280\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0615 - accuracy: 0.9895 - val_loss: 2.1776 - val_accuracy: 0.7840\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.0814 - accuracy: 0.9867 - val_loss: 1.3756 - val_accuracy: 0.8380\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0360 - accuracy: 0.9943 - val_loss: 1.2656 - val_accuracy: 0.8280\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0566 - accuracy: 0.9919 - val_loss: 1.1371 - val_accuracy: 0.8120\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0468 - accuracy: 0.9909 - val_loss: 1.3453 - val_accuracy: 0.8240\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0370 - accuracy: 0.9947 - val_loss: 1.1485 - val_accuracy: 0.8400\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 2s 61ms/step - loss: 0.0328 - accuracy: 0.9970 - val_loss: 1.0484 - val_accuracy: 0.8260\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 1.0462 - val_accuracy: 0.8380\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0277 - accuracy: 0.9970 - val_loss: 1.4049 - val_accuracy: 0.8260\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 2s 55ms/step - loss: 0.0156 - accuracy: 0.9988 - val_loss: 1.2621 - val_accuracy: 0.8320\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.0264 - accuracy: 0.9974 - val_loss: 1.5402 - val_accuracy: 0.8340\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 2s 53ms/step - loss: 0.0366 - accuracy: 0.9958 - val_loss: 1.4370 - val_accuracy: 0.8280\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0304 - accuracy: 0.9968 - val_loss: 1.2533 - val_accuracy: 0.8380\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0243 - accuracy: 0.9962 - val_loss: 1.2759 - val_accuracy: 0.8300\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 2s 49ms/step - loss: 0.0411 - accuracy: 0.9945 - val_loss: 1.6683 - val_accuracy: 0.8320\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0334 - accuracy: 0.9954 - val_loss: 1.4413 - val_accuracy: 0.8220\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 2s 50ms/step - loss: 0.0312 - accuracy: 0.9952 - val_loss: 1.6085 - val_accuracy: 0.8160\n",
      "16/16 - 0s - loss: 1.0724 - accuracy: 0.8580 - 140ms/epoch - 9ms/step\n",
      "INFO:tensorflow:Assets written to: model_logs/BiLSTM/model_0.858/assets\n",
      "Saved model: model_logs/BiLSTM/model_0.858\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import logging\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, Dropout, Dense, Embedding, Conv1D, LSTM, BatchNormalization\n",
    "\n",
    "# Create a directory for model logs\n",
    "if not os.path.exists('model_logs'):\n",
    "    os.mkdir('model_logs')\n",
    "\n",
    "model_folder = 'BiLSTM'\n",
    "\n",
    "# Create a directory for the current model\n",
    "model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "if not os.path.exists(model_logs_dir):\n",
    "    os.mkdir(model_logs_dir)\n",
    "\n",
    "# Configure logging to save results to a single log file\n",
    "log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "logging.basicConfig(filename=log_filepath,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_start_time)\n",
    "\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'embedding_size': [300],    # Different embedding sizes\n",
    "    'lstm_units': [32],         # Different LSTM units\n",
    "    'batch_size': [128],        # Different batch sizes\n",
    "}\n",
    "\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "\n",
    "# Iterate through the parameter combinations\n",
    "for params in param_combinations:\n",
    "    embedding_size = params['embedding_size']\n",
    "    lstm_units = params['lstm_units']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    print(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    \n",
    "    \n",
    "    bi_lstm = tf.keras.Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_seq_length, trainable=False),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        Conv1D(128, 5, activation='relu', padding='same'),              # Convolutional layer\n",
    "        GlobalMaxPooling1D(),                                           # Global Max Pooling\n",
    "        Dropout(0.2),                                                   # Dropout for regularization\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),     # Adding L2 regularization\n",
    "        BatchNormalization(),                                           # Batch normalization layer \n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "    initial_learning_rate = 0.01\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    bi_lstm.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define EarlyStopping callback to prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss', patience=10, restore_best_weights=True)\n",
    "    time_callback = TimeHistory()\n",
    "\n",
    "    # Train the model\n",
    "    history = bi_lstm.fit(train_data, train_labels, epochs=50,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(dev_data, dev_labels),\n",
    "                          callbacks=[early_stopping, time_callback],\n",
    "                        )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = bi_lstm.evaluate(\n",
    "        test_data, test_labels, verbose=2)\n",
    "    \n",
    "    \n",
    "    model_path = os.path.join(model_logs_dir, f'model_{round(test_accuracy,4)}')\n",
    "    \n",
    "    # Save the entire model\n",
    "    tf.keras.models.save_model(bi_lstm, model_path)\n",
    "    \n",
    "    model_info = {}\n",
    "\n",
    "    for layer in bi_lstm.layers:\n",
    "        # Layer name as the key\n",
    "        layer_name = layer.name\n",
    "        layer_info = {\n",
    "            'class_name': layer.__class__.__name__,\n",
    "            'config': layer.get_config(),  # Gets detailed configuration of the layer\n",
    "            'number_of_parameters': layer.count_params()\n",
    "        }\n",
    "        model_info[layer_name] = layer_info\n",
    "\n",
    "    # Now add this model_info to your 'info' dictionary\n",
    "    info = {\n",
    "        'Model': model_info,  # Detailed model information\n",
    "        'Hyperparameters': params,\n",
    "        'Test Loss': test_loss,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Epoch Data': epoch_data\n",
    "    }\n",
    "\n",
    "\n",
    "    info_path = os.path.join(\n",
    "        model_path, f'model_info.json')\n",
    "    with open(info_path, 'w') as info_file:\n",
    "        json.dump(info, info_file)\n",
    "\n",
    "    # Logging: You can log the saved model path and version\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "    # Log the results in the same log file\n",
    "    model_summary = []\n",
    "    bi_lstm.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_architecture = \"\\n\".join(model_summary)\n",
    "    logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "    logging.info(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    logging.info(f\"Test Loss: {test_loss}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + GRU + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(Attention, self).__init__()\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W_q = self.add_weight(\"W_q\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "#         self.W_k = self.add_weight(\"W_k\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "#         self.W_v = self.add_weight(\"W_v\", shape=(\n",
    "#             input_shape[-1], input_shape[-1]))\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         Q = tf.matmul(inputs, self.W_q)\n",
    "#         K = tf.matmul(inputs, self.W_k)\n",
    "#         V = tf.matmul(inputs, self.W_v)\n",
    "\n",
    "#         attention_scores = tf.matmul(Q, K, transpose_b=True)\n",
    "#         attention_scores = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "#         output = tf.matmul(attention_scores, V)\n",
    "#         return output\n",
    "\n",
    "# # Define a grid of hyperparameters to search\n",
    "# param_grid = {\n",
    "#     'embedding_size': [150],  # Different embedding sizes\n",
    "#     'lstm_units': [64],      # Different LSTM units\n",
    "#     'gru_units': [32],        # Different GRU units\n",
    "#     'batch_size': [64],      # Different batch sizes\n",
    "# }\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "# print('Param Combinations: ',param_combinations)\n",
    "# model_folder = 'Ensemble_Model'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Iterate through the parameter combinations\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     gru_units = params['gru_units']\n",
    "#     batch_size = params['batch_size']\n",
    "\n",
    "    \n",
    "#     print(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, GRU Units={gru_units}, Batch Size={batch_size}\")\n",
    "\n",
    "#     # Define your model with attention\n",
    "#     # Define your simpler model architecture\n",
    "#     attention_LGRB = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(\n",
    "#             input_dim=vocab_size,\n",
    "#             output_dim=embedding_size,\n",
    "#             input_length=max_seq_length,\n",
    "#             trainable=False\n",
    "#         ),\n",
    "#         # Dropout for the embedding layer\n",
    "#         tf.keras.layers.SpatialDropout1D(0.2),\n",
    "#         tf.keras.layers.LSTM(lstm_units, return_sequences=True,\n",
    "#                              kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # L2 regularization\n",
    "#         tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "#         tf.keras.layers.GRU(gru_units, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "#         Attention(),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(\n",
    "#             0.001)),  # L2 regularization\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(5, activation='softmax')\n",
    "#     ])\n",
    "\n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.1\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "#     attention_LGRB.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = attention_LGRB.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),)\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = attention_LGRB.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "\n",
    "#     # Log the results in the same log file\n",
    "#     model_summary = []\n",
    "#     attention_LGRB.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#     model_architecture = \"\\n\".join(model_summary)\n",
    "#     logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, GRU Units={gru_units}, Batch Size={batch_size}\")\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from tensorflow.keras.layers import Input, Conv1D,BatchNormalization, GlobalMaxPooling1D, Dropout, Dense, Embedding, Concatenate\n",
    "\n",
    "\n",
    "# class Attention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.W1 = tf.keras.layers.Dense(units)\n",
    "#         self.W2 = tf.keras.layers.Dense(units)\n",
    "#         self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "#     def call(self, features):\n",
    "#         query_value_attention_score = self.V(tf.nn.tanh(\n",
    "#             self.W1(features) + self.W2(features)))\n",
    "#         attention_weights = tf.nn.softmax(query_value_attention_score, axis=1)\n",
    "#         context_vector = attention_weights * features\n",
    "#         context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "#         return context_vector, attention_weights\n",
    "\n",
    "# # Updated hyperparameter grid to include depth and width\n",
    "# param_grid = {\n",
    "#     'embedding_size': [100],     # Embedding sizes\n",
    "#     'conv_depth': [1],          # Number of Conv1D layers (Depth)\n",
    "#     'conv_width': [128],    # Number of filters in each Conv1D layer (Width)\n",
    "#     'kernel_size': [5],         # Kernel sizes\n",
    "#     'batch_size': [64],      # Batch sizes\n",
    "#     'attention_units': [64]\n",
    "# }\n",
    "\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "# print('Param Combinations:')\n",
    "# for p in param_combinations:\n",
    "#     print(p)\n",
    "\n",
    "# model_folder = 'CNN'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # In the model architecture:\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     conv_depth = params['conv_depth']\n",
    "#     conv_width = params['conv_width']\n",
    "#     kernel_size = params['kernel_size']\n",
    "#     batch_size = params['batch_size']\n",
    "#     attention_units = params['attention_units']\n",
    "#     # Log the current hyperparameters\n",
    "#     print(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Depth={conv_depth}, Conv Width={conv_width}, Kernel Size={kernel_size}, Batch Size={batch_size}\")\n",
    "\n",
    "#     # -------------------------------------------------------------------------------------\n",
    "#     # input_layer = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "#     # embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "#     #                             output_dim=embedding_size,\n",
    "#     #                             input_length=max_seq_length,\n",
    "#     #                             trainable=False)(input_layer)\n",
    "#     # conv_layer = tf.keras.layers.Conv1D(conv_width, 5, activation='relu')(embedding_layer)\n",
    "#     # context_vector, attention_weights = Attention(attention_units)(conv_layer)\n",
    "#     # dense_layer = tf.keras.layers.Dense(64, activation='relu')(context_vector)\n",
    "#     # dropout_layer = tf.keras.layers.Dropout(0.6)(dense_layer)\n",
    "#     # output_layer = tf.keras.layers.Dense(5, activation='softmax')(dropout_layer)\n",
    "\n",
    "#     # conv_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "#     # -------------------------------------------------------------------------------------\n",
    "#     input_layer = Input(shape=(max_seq_length,))\n",
    "#     embedding_layer = Embedding(input_dim=vocab_size,\n",
    "#                     output_dim=embedding_size,\n",
    "#                     input_length=max_seq_length,\n",
    "#                     trainable=False)(input_layer)\n",
    "#     conv_layer_1 = Conv1D(filters=conv_width, kernel_size=3, activation='relu', padding='same')(embedding_layer)\n",
    "#     conv_layer_2 = Conv1D(filters=conv_width, kernel_size=4, activation='relu', padding='same')(conv_layer_1)\n",
    "#     conv_layer_3 = Conv1D(filters=conv_width, kernel_size=5, activation='relu', padding='same')(conv_layer_2)\n",
    "#     bn_layer_1 = BatchNormalization()(conv_layer_1)\n",
    "#     bn_layer_2 = BatchNormalization()(conv_layer_2)\n",
    "#     bn_layer_3 = BatchNormalization()(conv_layer_3)\n",
    "#     concat_layer = Concatenate()([bn_layer_1, bn_layer_2, bn_layer_3])\n",
    "#     pooling_layer = GlobalMaxPooling1D()(concat_layer)\n",
    "#     context_vector, attention_weights = Attention(attention_units)(tf.expand_dims(pooling_layer, 1))\n",
    "#     dense_layer = Dense(128, activation='relu')(context_vector)\n",
    "#     dropout_layer = Dropout(0.5)(dense_layer)\n",
    "#     output_layer = Dense(5, activation='softmax')(dropout_layer)\n",
    "    \n",
    "#     conv_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.01\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "   \n",
    "#     conv_model.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = conv_model.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),\n",
    "#                         callbacks=[early_stopping],\n",
    "#                         workers=8,\n",
    "#                         )\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = conv_model.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Depth={conv_depth}, Conv Width={conv_width}, Kernel Size={kernel_size}, Batch Size={batch_size}\")\n",
    "    \n",
    "#     model_path = os.path.join(model_logs_dir, f'model_{round(test_accuracy,4)}')\n",
    "\n",
    "#     # Save the entire model\n",
    "#     tf.keras.models.save_model(conv_model, model_path)\n",
    "\n",
    "#     # Record additional information in a JSON file\n",
    "#     info = {\n",
    "#         'Model':str(conv_model.summary()),\n",
    "#         'Hyperparameters': params,\n",
    "#         'Test Loss': test_loss,\n",
    "#         'Test Accuracy': test_accuracy\n",
    "#     }\n",
    "\n",
    "#     info_path = os.path.join(\n",
    "#         model_path, f'model__info.json')\n",
    "#     with open(info_path, 'w') as info_file:\n",
    "#         json.dump(info, info_file)\n",
    "\n",
    "#     # Logging: You can log the saved model path and version\n",
    "#     print(f\"Saved model: {model_path}\")\n",
    "    \n",
    "#     # Log the results in the same log file\n",
    "#     model_summary = []\n",
    "#     conv_model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#     model_architecture = \"\\n\".join(model_summary)\n",
    "#     logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\\n#################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XAI for chosen model explainaibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_seq_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "def predict_fn(texts):\n",
    "    preprocessed_texts = preprocess_text(texts)\n",
    "    return bi_lstm.predict(preprocessed_texts)\n",
    "\n",
    "\n",
    "# Create a LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=list(label_encoder.keys()))\n",
    "\n",
    "# Choose an instance from your dataset\n",
    "# Replace with an actual text from your dataset\n",
    "instance = \"What American composer wrote the music for `` West Side Story '' ?\"\n",
    "\n",
    "# Generate an explanation\n",
    "explanation = explainer.explain_instance(\n",
    "    instance, predict_fn, num_features=10, top_labels=3)\n",
    "\n",
    "# Show the explanation for the top class\n",
    "explanation.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# # Define a grid of hyperparameters to search\n",
    "# param_grid = {\n",
    "#     'embedding_size': [100, 300],\n",
    "#     'conv_width': [128],\n",
    "#     'lstm_units': [64],\n",
    "#     'batch_size': [64],\n",
    "#     'dense_units': [64, 256],\n",
    "#     'kernel_size': [5, 7]\n",
    "# }\n",
    "\n",
    "# param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "# print('Param Combinations: ')\n",
    "# for p in param_combinations:\n",
    "#     print(p)\n",
    "\n",
    "# model_folder = 'CNN+LSTM'\n",
    "\n",
    "# # Create a directory for the current model\n",
    "# model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "# if not os.path.exists(model_logs_dir):\n",
    "#     os.mkdir(model_logs_dir)\n",
    "\n",
    "# # Configure logging to save results to a single log file\n",
    "# log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "# logging.basicConfig(filename=log_filepath,\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# logged_model = False\n",
    "# # In the model architecture:\n",
    "# for params in param_combinations:\n",
    "#     embedding_size = params['embedding_size']\n",
    "#     conv_width = params['conv_width']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     dense_units = params['dense_units']\n",
    "#     batch_size = params['batch_size']\n",
    "#     kernel_size = params['kernel_size']\n",
    "\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "#                                   output_dim=embedding_size,\n",
    "#                                   input_length=max_seq_length,\n",
    "#                                   trainable=False),\n",
    "#         tf.keras.layers.Conv1D(conv_width, kernel_size, activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#         tf.keras.layers.LSTM(lstm_units, return_sequences=True),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         tf.keras.layers.Dense(dense_units, activation='relu',\n",
    "#                               kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dense(5, activation='softmax')\n",
    "#     ])\n",
    "#     # Log the results in the same log file\n",
    "#     if not logged_model:\n",
    "#         model_summary = []\n",
    "#         model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "#         model_architecture = \"\\n\".join(model_summary)\n",
    "#         logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "#         logged_model = True\n",
    "#     # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "#     initial_learning_rate = 0.01\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Define EarlyStopping callback to prevent overfitting\n",
    "#     early_stopping = EarlyStopping(\n",
    "#         monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = model.fit(train_data, train_labels, epochs=50,\n",
    "#                         batch_size=batch_size,\n",
    "#                         validation_data=(dev_data, dev_labels),\n",
    "#                         callbacks=[early_stopping])\n",
    "\n",
    "#     # Evaluate the model on the test se\n",
    "#     test_loss, test_accuracy = model.evaluate(\n",
    "#         test_data, test_labels, verbose=2)\n",
    "\n",
    "   \n",
    "#     logging.info(\n",
    "#         f\"Testing hyperparameters: Embedding Size={embedding_size}, Conv Width={conv_width}, LSTM Units={lstm_units}, Dense Units={dense_units}, Batch Size={batch_size}\")\n",
    "#     logging.info(f\"Test Loss: {test_loss}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy}\\n#####################################################################################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
