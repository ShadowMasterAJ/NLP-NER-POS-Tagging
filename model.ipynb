{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Related third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization, Bidirectional, Conv1D, Dense, Dropout, Embedding, GlobalMaxPooling1D, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "1. Load the embedding matrix and tokenizer. Tokenise the data using the tokeniser\n",
    "\n",
    "2. Convert text data from three different data frames (train_df, dev_df, and test_df) into sequences of integers. Each word in the text is replaced by its corresponding index from the word index created earlier.\n",
    "\n",
    "3. Set a maximum sequence length (max_seq_length) and then pad the sequences to ensure that they all have the same length. \\\n",
    "    a. Padding is done with zeros (`0.`)\n",
    "\n",
    "4. Create a label encoder by mapping unique labels in the 'label-coarse' column of the training data frame to integers.\n",
    "\n",
    "5. Encode the labels for the training, development, and test data sets using this label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reduced embedding matrix\n",
    "embedding_matrix = np.load('models/embedding_matrix.npy')\n",
    "vocab_size, embedding_size = embedding_matrix.shape\n",
    "\n",
    "# Load saved tokenizer\n",
    "with open('models/tokenizer.json') as f:\n",
    "    tokenizer_data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_data)\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_csv('TREC_dataset/train.csv')\n",
    "dev_df = pd.read_csv('TREC_dataset/dev.csv')\n",
    "test_df = pd.read_csv('TREC_dataset/test.csv')\n",
    "\n",
    "# Convert text data to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad sequences to ensure they have the same length\n",
    "max_seq_length = 40  # Adjust as needed\n",
    "\n",
    "train_data = pad_sequences(\n",
    "    train_sequences, maxlen=max_seq_length, padding='post')\n",
    "dev_data = pad_sequences(dev_sequences, maxlen=max_seq_length, padding='post')\n",
    "test_data = pad_sequences(\n",
    "    test_sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = {label: i for i, label in enumerate(train_df['label-coarse'].unique())}\n",
    "\n",
    "train_labels = np.array([label_encoder[label]for label in train_df['label-coarse']])\n",
    "dev_labels = np.array([label_encoder[label]for label in dev_df['label-coarse']])\n",
    "test_labels = np.array([label_encoder[label]for label in test_df['label-coarse']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM + CNN [FINALISED]\n",
    "\n",
    "### Step 1: Define Hyperparameter Grid\n",
    "\n",
    "1. Define a grid of hyperparameters to search, including:\n",
    "   - `embedding_size`: Different embedding sizes.\n",
    "   - `lstm_units`: Different LSTM units.\n",
    "   - `batch_size`: Different batch sizes.\n",
    "\n",
    "### Step 2: Logging and Model Directory Setup\n",
    "\n",
    "2. Create a directory structure to store model logs.\n",
    "3. Configure logging to track and save results to a log file.\n",
    "\n",
    "### Step 3: Model Training Loop\n",
    "\n",
    "In this step, we systematically train multiple models, each with different hyperparameter settings, to identify the best-performing configuration. The process is as follows:\n",
    "\n",
    "#### Substep 1: Hyperparameter Iteration\n",
    "\n",
    "4.1. Iterate through the predefined hyperparameter combinations.\n",
    "   - For each combination, we explore various settings for:\n",
    "     - `embedding_size`: The size of word embeddings.\n",
    "     - `lstm_units`: The number of LSTM units in the bidirectional LSTM layer.\n",
    "     - `batch_size`: The batch size used during training.\n",
    "\n",
    "#### Substep 2: Model Architecture\n",
    "\n",
    "4.2. Build the model architecture for the current hyperparameter combination. The architecture includes:\n",
    "   - Embedding Layer: Converts input sequences into dense vector representations.\n",
    "   - Bidirectional LSTM: A recurrent layer that captures contextual information bidirectionally.\n",
    "   - Convolutional Layer: Applies convolutional operations to capture local patterns.\n",
    "   - Global Max Pooling: Extracts the most relevant information from convolutional outputs.\n",
    "   - Dropout: Regularization technique to prevent overfitting.\n",
    "   - Dense Layers: Fully connected layers for classification.\n",
    "   \n",
    "#### Substep 3: Model Compilation\n",
    "\n",
    "4.3. Compile the model with the following configurations:\n",
    "   - Learning Rate Schedule: Uses an exponential decay schedule to adjust the learning rate.\n",
    "   - Optimizer: Utilizes the Adam optimizer for gradient descent.\n",
    "   \n",
    "#### Substep 4: Early Stopping\n",
    "\n",
    "4.4. Implement early stopping as a precautionary measure to prevent overfitting during training. Early stopping monitors the loss on the training set and stops training if the loss on the training set does not improve for a specified number of epochs.\n",
    "\n",
    "#### Substep 5: Class Weights\n",
    "\n",
    "4.5. Define class weights to address data imbalance issues. Class weights assign higher importance to underrepresented classes during training, helping the model better learn from imbalanced data.\n",
    "\n",
    "#### Substep 6: Training and Validation\n",
    "\n",
    "4.6. Train the model on the training dataset with the specified hyperparameters. During training, the model learns to make predictions based on input sequences. Validation is performed on a separate development dataset to assess the model's performance during training.\n",
    "\n",
    "By systematically exploring different hyperparameter combinations and training models with varying configurations, we aim to identify the best-performing model with the most suitable hyperparameters for the text classification task.\n",
    "\n",
    "\n",
    "### Step 4: Model Saving\n",
    "\n",
    "5. After training, evaluate the model on the test set.\n",
    "6. Save the model in a directory named based on its test accuracy (rounded to four decimal places).\n",
    "7. Record additional information in a JSON file, including the model summary.\n",
    "8. Log the saved model path and version.\n",
    "\n",
    "Summary\n",
    "\n",
    "This code demonstrates a systematic approach to hyperparameter tuning and model saving for text classification tasks, ensuring reproducibility and easy tracking of model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for model logs\n",
    "if not os.path.exists('model_logs'):\n",
    "    os.mkdir('model_logs')\n",
    "\n",
    "model_folder = 'BiLSTM'\n",
    "\n",
    "# Create a directory for the current model\n",
    "model_logs_dir = os.path.join('model_logs', model_folder)\n",
    "if not os.path.exists(model_logs_dir):\n",
    "    os.mkdir(model_logs_dir)\n",
    "\n",
    "# Configure logging to save results to a single log file\n",
    "log_filepath = os.path.join(model_logs_dir, 'model_log.txt')\n",
    "logging.basicConfig(filename=log_filepath,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'embedding_size': [300],    # Different embedding sizes\n",
    "    'lstm_units': [32],         # Different LSTM units\n",
    "    'batch_size': [128],        # Different batch sizes\n",
    "}\n",
    "\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Iterate through the parameter combinations\n",
    "for params in param_combinations:\n",
    "    embedding_size = params['embedding_size']\n",
    "    lstm_units = params['lstm_units']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    print(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    \n",
    "    bi_lstm = tf.keras.Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_seq_length, trainable=False),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        Conv1D(128, 5, activation='relu', padding='same'),              # Convolutional layer\n",
    "        GlobalMaxPooling1D(),                                           # Global Max Pooling\n",
    "        Dropout(0.2),                                                   # Dropout for regularization\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),     # Adding L2 regularization\n",
    "        BatchNormalization(),                                           # Batch normalization layer \n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with a lower initial learning rate and learning rate scheduler\n",
    "    initial_learning_rate = 0.01\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,decay_steps=1000, decay_rate=0.9)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    bi_lstm.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define EarlyStopping callback to prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss', patience=5, restore_best_weights=True)\n",
    "    time_callback = TimeHistory()\n",
    "\n",
    "    # Train the model\n",
    "    history = bi_lstm.fit(train_data, train_labels, epochs=50,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(dev_data, dev_labels),\n",
    "                          callbacks=[early_stopping, time_callback],\n",
    "                        )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = bi_lstm.evaluate(\n",
    "        test_data, test_labels, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model_path = os.path.join(model_logs_dir, f'model_{round(test_accuracy,4)}')\n",
    "    \n",
    "    # Save the entire model\n",
    "    tf.keras.models.save_model(bi_lstm, model_path)\n",
    "    \n",
    "    model_info = {}\n",
    "\n",
    "    epoch_data = {\n",
    "        'epoch': list(range(1, len(history.history['accuracy']) + 1)),\n",
    "        'timing': time_callback.times,\n",
    "        'train_loss': history.history['loss'],\n",
    "        'train_accuracy': history.history['accuracy'],\n",
    "        'val_loss': history.history['val_loss'],\n",
    "        'val_accuracy': history.history['val_accuracy']\n",
    "    }\n",
    "    \n",
    "    for layer in bi_lstm.layers:\n",
    "        # Layer name as the key\n",
    "        layer_name = layer.name\n",
    "        layer_info = {\n",
    "            'class_name': layer.__class__.__name__,\n",
    "            'config': layer.get_config(),  # Gets detailed configuration of the layer\n",
    "            'number_of_parameters': layer.count_params()\n",
    "        }\n",
    "        model_info[layer_name] = layer_info\n",
    "     \n",
    "     \n",
    "    # Now add this model_info to your 'info' dictionary\n",
    "    info = {\n",
    "        'Model': model_info,  # Detailed model information\n",
    "        'Hyperparameters': params,\n",
    "        'Test Loss': test_loss,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Epoch Data': epoch_data\n",
    "    }\n",
    "\n",
    "\n",
    "    info_path = os.path.join(\n",
    "        model_path, f'model_info.json')\n",
    "    with open(info_path, 'w') as info_file:\n",
    "        json.dump(info, info_file)\n",
    "\n",
    "    # Logging: You can log the saved model path and version\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "    # Log the results in the same log file\n",
    "    model_summary = []\n",
    "    bi_lstm.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_architecture = \"\\n\".join(model_summary)\n",
    "    logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "    logging.info(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    logging.info(f\"Test Loss: {test_loss}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    info_path = os.path.join(\n",
    "        model_path, f'model_info.json')\n",
    "    with open(info_path, 'w') as info_file:\n",
    "        json.dump(info, info_file)\n",
    "\n",
    "    # Logging: You can log the saved model path and version\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "    # Log the results in the same log file\n",
    "    model_summary = []\n",
    "    bi_lstm.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_architecture = \"\\n\".join(model_summary)\n",
    "    logging.info(\"Model Architecture:\\n\" + model_architecture)\n",
    "    logging.info(\n",
    "        f\"Testing hyperparameters: Embedding Size={embedding_size}, LSTM Units={lstm_units}, Batch Size={batch_size}\")\n",
    "    logging.info(f\"Test Loss: {test_loss}\")\n",
    "    logging.info(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm  = tf.keras.models.load_model('model_logs/BiLSTM/model_0.85')\n",
    "\n",
    "tf.keras.utils.plot_model(bi_lstm,\n",
    "                          show_shapes=True,\n",
    "                          show_trainable=True,\n",
    "                          show_layer_activations=True,\n",
    "                          show_dtype=True,\n",
    "                          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
